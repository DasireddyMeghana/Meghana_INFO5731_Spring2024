{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DasireddyMeghana/Meghana_INFO5731_Spring2024/blob/main/In_class_exercise/Dasireddy_Meghana_Exercise_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DymRJbxDBCnf"
      },
      "source": [
        "# **INFO5731 In-class Exercise 2**\n",
        "\n",
        "The purpose of this exercise is to understand users' information needs, and then collect data from different sources for analysis by implementing web scraping using Python.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission. , and no requests will be answered. Manage your time accordingly.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1 (10 Points)\n",
        "Describe an interesting research question (or practical question or something innovative) you have in mind, what kind of data should be collected to answer the question(s)? Specify the amount of data needed for analysis. Provide detailed steps for collecting and saving the data."
      ],
      "metadata": {
        "id": "FBKvD6O_TY6e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "r7RkYVG5MNb7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Research Question:\n",
        "\n",
        "Investigate the impact of different types of social media content on user engagement in the context of mental health awareness campaigns. Specifically, we aim to understand whether certain types of content has more attention, elicit more engagement, and contribute to increased awareness and discussions about mental health issues.\n",
        "\n",
        "\n",
        "Data Collection Plan:\n",
        "\n",
        "1. Identify Relevant Subreddits: Select subreddits related to mental health, such as r/mentalhealth, r/depression, or r/Anxiety, where users often share personal experiences, resources, and discussions related to mental health.\n",
        "\n",
        "2. Define Search Keywords: Determine keywords relevant to mental health topics, such as \"depression,\" \"anxiety,\" \"mental health awareness,\" \"therapy,\" etc., to capture a diverse range of content.\n",
        "\n",
        "3. Collect Data: Utilize the provided Python code with modifications to fetch posts from the selected subreddits using the identified search keywords. Collect various types of content, including text-based posts, images, and videos, to analyze their impact on user engagement.\n",
        "\n",
        "4. Data Filtering: Filter the collected data to ensure relevance and eliminate irrelevant or off-topic posts. This step is crucial to maintain the quality and focus of the dataset.\n",
        "\n",
        "5. Data Annotation: Depending on the research requirements, consider annotating the collected data with additional information, such as sentiment analysis of text-based posts, image analysis, or video categorization.\n",
        "\n",
        "6. Save Data: Save the collected and filtered dataset in a structured format, such as CSV for further analysis. Ensure that each entry includes relevant information such as post title, content type, engagement metrics, timestamp, and author details.\n",
        "'''"
      ],
      "metadata": {
        "id": "uTKkFyAJNshl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2 (10 Points)\n",
        "Write Python code to collect a dataset of 1000 samples related to the question discussed in Question 1."
      ],
      "metadata": {
        "id": "E9RqrlwdTfvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import praw\n",
        "import datetime\n",
        "import pandas as pd\n",
        "\n",
        "# Initialize Reddit instance with your credentials\n",
        "reddit = praw.Reddit(client_id='Rm51RgkkORs6zhyg23rNcg', client_secret='G4yNrSb24cbkLUXm5frLVOxjji18Bg', user_agent='test')\n",
        "\n",
        "def fetch_posts_from_subreddit(subreddit_name, search_keyword, limit=1000):\n",
        "    fetched_posts = []\n",
        "    subreddit = reddit.subreddit(subreddit_name)\n",
        "\n",
        "    # Iterate through submissions in the subreddit\n",
        "    for submission in subreddit.search(search_keyword, limit=limit):\n",
        "        post_info = {\n",
        "            'title': submission.title,\n",
        "            'content_type': submission.post_hint if hasattr(submission, 'post_hint') else \"text\",\n",
        "            'url': submission.url,\n",
        "            'created_utc': datetime.datetime.utcfromtimestamp(submission.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n",
        "            'upvotes': submission.score,\n",
        "            'num_comments': submission.num_comments,\n",
        "            'author': str(submission.author)\n",
        "        }\n",
        "        fetched_posts.append(post_info)\n",
        "\n",
        "    return fetched_posts\n",
        "\n",
        "target_subreddit = 'mentalhealth'\n",
        "search_query = 'mental health'\n",
        "retrieved_posts = fetch_posts_from_subreddit(target_subreddit, search_query, limit=1000)\n",
        "\n",
        "# Save the collected data to a CSV file\n",
        "import csv\n",
        "\n",
        "# Define CSV file path\n",
        "csv_file_path = 'mental_health_posts.csv'\n",
        "\n",
        "# Write data to CSV file\n",
        "with open(csv_file_path, mode='w', newline='', encoding='utf-8') as file:\n",
        "    fieldnames = ['title', 'content_type', 'url', 'created_utc', 'upvotes', 'num_comments', 'author']\n",
        "    writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
        "\n",
        "    writer.writeheader()\n",
        "    for post in retrieved_posts:\n",
        "        writer.writerow(post)\n",
        "\n",
        "print(\"Data collection and saving completed successfully.\")\n",
        "df = pd.read_csv(\"mental_health_posts.csv\")\n",
        "print(df)\n"
      ],
      "metadata": {
        "id": "4XvRknixTh1g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0664d556-1741-466a-e6d0-d9494f20fe80"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data collection and saving completed successfully.\n",
            "                                               title content_type  \\\n",
            "0  Do people without any mental health issues act...         text   \n",
            "1  It's World's Mental Health Day today. If you c...         text   \n",
            "2  My wife has several mental health issues, and ...         text   \n",
            "3  unpopular opinion - mental health should be ta...         text   \n",
            "4  Anyone else feel \"too old\" to have mental heal...         text   \n",
            "5  What usually tried to shatter your mental health?         text   \n",
            "6  Does everyone really struggle with their menta...         text   \n",
            "7                                      Mental health         text   \n",
            "8  what positive mental health actions did you ta...         text   \n",
            "9  Please Stop Recommending Pets as a Mental Heal...         text   \n",
            "\n",
            "                                                 url          created_utc  \\\n",
            "0  https://www.reddit.com/r/mentalhealth/comments...  2023-10-11 11:34:53   \n",
            "1  https://www.reddit.com/r/mentalhealth/comments...  2023-10-10 19:50:25   \n",
            "2  https://www.reddit.com/r/mentalhealth/comments...  2024-02-09 01:48:44   \n",
            "3  https://www.reddit.com/r/mentalhealth/comments...  2022-11-12 17:24:00   \n",
            "4  https://www.reddit.com/r/mentalhealth/comments...  2023-10-26 18:49:34   \n",
            "5  https://www.reddit.com/r/mentalhealth/comments...  2023-10-25 01:00:14   \n",
            "6  https://www.reddit.com/r/mentalhealth/comments...  2023-11-13 14:02:29   \n",
            "7  https://www.reddit.com/r/mentalhealth/comments...  2022-10-16 07:17:07   \n",
            "8  https://www.reddit.com/r/mentalhealth/comments...  2021-11-10 20:50:34   \n",
            "9  https://www.reddit.com/r/mentalhealth/comments...  2023-10-04 18:52:42   \n",
            "\n",
            "   upvotes  num_comments               author  \n",
            "0      552           191    Wild-Storage-1663  \n",
            "1      249           228            MehhQueen  \n",
            "2      352           174      Ok_Practice2701  \n",
            "3      807           159   someone_elses_idea  \n",
            "4      198           142            zoonthego  \n",
            "5       52           150       Grouchy_Ad6730  \n",
            "6      139            87       someoneinlife1  \n",
            "7      113           106  sudocreamcancerbabe  \n",
            "8      330           318            Andra8951  \n",
            "9      284            54            dejamoo75  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03jb4GZsBkBS"
      },
      "source": [
        "## Question 3 (10 Points)\n",
        "Write Python code to collect 1000 articles from Google Scholar (https://scholar.google.com/), Microsoft Academic (https://academic.microsoft.com/home), or CiteSeerX (https://citeseerx.ist.psu.edu/index), or Semantic Scholar (https://www.semanticscholar.org/), or ACM Digital Libraries (https://dl.acm.org/) with the keyword \"XYZ\". The articles should be published in the last 10 years (2014-2024).\n",
        "\n",
        "The following information from the article needs to be collected:\n",
        "\n",
        "(1) Title of the article\n",
        "\n",
        "(2) Venue/journal/conference being published\n",
        "\n",
        "(3) Year\n",
        "\n",
        "(4) Authors\n",
        "\n",
        "(5) Abstract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "YaGLbSHHB8Ej",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "535c69e3-b764-4c6c-d461-1a99e0b21ab6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'title': 'Lecture Notes: Optimization for Machine Learning', 'venue/journal/conference': 'arxiv', 'year': '2019', 'authors': 'Elad Hazan', 'abstract': 'Lecture notes on optimization for machine learning, derived from a course at\\nPrinceton University and tutorials given in MLSS, Buenos Aires, as well as\\nSimons Foundation, Berkeley.'}\n",
            "{'title': 'An Optimal Control View of Adversarial Machine Learning', 'venue/journal/conference': 'arxiv', 'year': '2018', 'authors': 'Xiaojin Zhu', 'abstract': \"I describe an optimal control view of adversarial machine learning, where the\\ndynamical system is the machine learner, the input are adversarial actions, and\\nthe control costs are defined by the adversary's goals to do harm and be hard\\nto detect. This view encompasses many types of adversarial machine learning,\\nincluding test-item attacks, training-data poisoning, and adversarial reward\\nshaping. The view encourages adversarial machine learning researcher to utilize\\nadvances in control theory and reinforcement learning.\"}\n",
            "{'title': 'Minimax deviation strategies for machine learning and recognition with\\n  short learning samples', 'venue/journal/conference': 'arxiv', 'year': '2017', 'authors': 'Michail Schlesinger, Evgeniy Vodolazskiy', 'abstract': 'The article is devoted to the problem of small learning samples in machine\\nlearning. The flaws of maximum likelihood learning and minimax learning are\\nlooked into and the concept of minimax deviation learning is introduced that is\\nfree of those flaws.'}\n",
            "{'title': 'Machine Learning for Clinical Predictive Analytics', 'venue/journal/conference': 'arxiv', 'year': '2019', 'authors': 'Wei-Hung Weng', 'abstract': 'In this chapter, we provide a brief overview of applying machine learning\\ntechniques for clinical prediction tasks. We begin with a quick introduction to\\nthe concepts of machine learning and outline some of the most common machine\\nlearning algorithms. Next, we demonstrate how to apply the algorithms with\\nappropriate toolkits to conduct machine learning experiments for clinical\\nprediction tasks. The objectives of this chapter are to (1) understand the\\nbasics of machine learning techniques and the reasons behind why they are\\nuseful for solving clinical prediction problems, (2) understand the intuition\\nbehind some machine learning models, including regression, decision trees, and\\nsupport vector machines, and (3) understand how to apply these models to\\nclinical prediction problems using publicly available datasets via case\\nstudies.'}\n",
            "{'title': 'Towards Modular Machine Learning Solution Development: Benefits and\\n  Trade-offs', 'venue/journal/conference': 'arxiv', 'year': '2023', 'authors': 'Samiyuru Menik, Lakshmish Ramaswamy', 'abstract': \"Machine learning technologies have demonstrated immense capabilities in\\nvarious domains. They play a key role in the success of modern businesses.\\nHowever, adoption of machine learning technologies has a lot of untouched\\npotential. Cost of developing custom machine learning solutions that solve\\nunique business problems is a major inhibitor to far-reaching adoption of\\nmachine learning technologies. We recognize that the monolithic nature\\nprevalent in today's machine learning applications stands in the way of\\nefficient and cost effective customized machine learning solution development.\\nIn this work we explore the benefits of modular machine learning solutions and\\ndiscuss how modular machine learning solutions can overcome some of the major\\nsolution engineering limitations of monolithic machine learning solutions. We\\nanalyze the trade-offs between modular and monolithic machine learning\\nsolutions through three deep learning problems; one text based and the two\\nimage based. Our experimental results show that modular machine learning\\nsolutions have a promising potential to reap the solution engineering\\nadvantages of modularity while gaining performance and data advantages in a way\\nthe monolithic machine learning solutions do not permit.\"}\n",
            "{'title': 'Recent Advances in Optimal Transport for Machine Learning', 'venue/journal/conference': 'arxiv', 'year': '2023', 'authors': 'Eduardo Fernandes Montesuma, Fred Ngolè Mboula, Antoine Souloumiac', 'abstract': 'Recently, Optimal Transport has been proposed as a probabilistic framework in\\nMachine Learning for comparing and manipulating probability distributions. This\\nis rooted in its rich history and theory, and has offered new solutions to\\ndifferent problems in machine learning, such as generative modeling and\\ntransfer learning. In this survey we explore contributions of Optimal Transport\\nfor Machine Learning over the period 2012 -- 2022, focusing on four sub-fields\\nof Machine Learning: supervised, unsupervised, transfer and reinforcement\\nlearning. We further highlight the recent development in computational Optimal\\nTransport, and its interplay with Machine Learning practice.'}\n",
            "{'title': 'Introduction to Machine Learning: Class Notes 67577', 'venue/journal/conference': 'arxiv', 'year': '2009', 'authors': 'Amnon Shashua', 'abstract': 'Introduction to Machine learning covering Statistical Inference (Bayes, EM,\\nML/MaxEnt duality), algebraic and spectral methods (PCA, LDA, CCA, Clustering),\\nand PAC learning (the Formal model, VC dimension, Double Sampling theorem).'}\n",
            "{'title': 'The Tribes of Machine Learning and the Realm of Computer Architecture', 'venue/journal/conference': 'arxiv', 'year': '2020', 'authors': 'Ayaz Akram, Jason Lowe-Power', 'abstract': 'Machine learning techniques have influenced the field of computer\\narchitecture like many other fields. This paper studies how the fundamental\\nmachine learning techniques can be applied towards computer architecture\\nproblems. We also provide a detailed survey of computer architecture research\\nthat employs different machine learning methods. Finally, we present some\\nfuture opportunities and the outstanding challenges that need to be overcome to\\nexploit full potential of machine learning for computer architecture.'}\n",
            "{'title': 'A Machine Learning Tutorial for Operational Meteorology, Part I:\\n  Traditional Machine Learning', 'venue/journal/conference': 'arxiv', 'year': '2022', 'authors': 'Randy J. Chase, David R. Harrison, Amanda Burke, Gary M. Lackmann, Amy McGovern', 'abstract': \"Recently, the use of machine learning in meteorology has increased greatly.\\nWhile many machine learning methods are not new, university classes on machine\\nlearning are largely unavailable to meteorology students and are not required\\nto become a meteorologist. The lack of formal instruction has contributed to\\nperception that machine learning methods are 'black boxes' and thus end-users\\nare hesitant to apply the machine learning methods in their every day workflow.\\nTo reduce the opaqueness of machine learning methods and lower hesitancy\\ntowards machine learning in meteorology, this paper provides a survey of some\\nof the most common machine learning methods. A familiar meteorological example\\nis used to contextualize the machine learning methods while also discussing\\nmachine learning topics using plain language. The following machine learning\\nmethods are demonstrated: linear regression; logistic regression; decision\\ntrees; random forest; gradient boosted decision trees; naive Bayes; and support\\nvector machines. Beyond discussing the different methods, the paper also\\ncontains discussions on the general machine learning process as well as best\\npractices to enable readers to apply machine learning to their own datasets.\\nFurthermore, all code (in the form of Jupyter notebooks and Google Colaboratory\\nnotebooks) used to make the examples in the paper is provided in an effort to\\ncatalyse the use of machine learning in meteorology.\"}\n",
            "{'title': 'Position Paper: Towards Transparent Machine Learning', 'venue/journal/conference': 'arxiv', 'year': '2019', 'authors': 'Dustin Juliano', 'abstract': 'Transparent machine learning is introduced as an alternative form of machine\\nlearning, where both the model and the learning system are represented in\\nsource code form. The goal of this project is to enable direct human\\nunderstanding of machine learning models, giving us the ability to learn,\\nverify, and refine them as programs. If solved, this technology could represent\\na best-case scenario for the safety and security of AI systems going forward.'}\n",
            "Got 10 articles\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import xml.etree.ElementTree as ET\n",
        "import time\n",
        "\n",
        "def fetch_arxiv_articles(keyword, max_results=1000):\n",
        "    \"\"\"Fetches articles from arXiv matching the given keyword.\n",
        "\n",
        "    Args:\n",
        "        keyword: The keyword to search for.\n",
        "        max_results: The maximum number of articles to fetch (default: 1000).\n",
        "\n",
        "    Returns:\n",
        "        A list of dictionaries, where each dictionary represents an article.\n",
        "    \"\"\"\n",
        "\n",
        "    base_url = \"http://export.arxiv.org/api/query?\"\n",
        "    search_url = \"{http://www.w3.org/2005/Atom}\"\n",
        "    articles = []\n",
        "    start = 0\n",
        "    max_per_query = 100\n",
        "\n",
        "    while len(articles) < max_results:\n",
        "        query_params = f\"search_query=all:{keyword}&start={start}&max_results={max_per_query}\"\n",
        "        response = requests.get(base_url + query_params)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            root = ET.fromstring(response.content)\n",
        "            for entry in root.findall(search_url + 'entry'):\n",
        "                article = {\n",
        "                    \"title\": entry.find(search_url + 'title').text.strip(),\n",
        "                    \"venue/journal/conference\": \"arxiv\",\n",
        "                    \"year\": entry.find(search_url + 'published').text[:4],  # Extract year\n",
        "                    \"authors\": \", \".join([author.find(search_url + 'name').text for author in entry.findall(search_url + 'author')]),\n",
        "                    \"abstract\": entry.find(search_url + 'summary').text.strip()\n",
        "                }\n",
        "                articles.append(article)\n",
        "\n",
        "                if len(articles) >= max_results:\n",
        "                    break\n",
        "            start += max_per_query\n",
        "        else:\n",
        "            print(f\"Failed to get the data: {response.status_code}\")\n",
        "            break\n",
        "\n",
        "        time.sleep(3)\n",
        "\n",
        "    return articles\n",
        "\n",
        "keyword = \"Machine Learning\"\n",
        "articles = fetch_arxiv_articles(keyword, 1000)\n",
        "for article in articles:\n",
        "    print(article)\n",
        "print(f\"Got {len(articles)} articles\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJDe71iLB616"
      },
      "source": [
        "## Question 4A (10 Points)\n",
        "Develop Python code to collect data from social media platforms like Reddit, Instagram, Twitter (formerly known as X), Facebook, or any other. Use hashtags, keywords, usernames, or user IDs to gather the data.\n",
        "\n",
        "\n",
        "\n",
        "Ensure that the collected data has more than four columns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "MtKskTzbCLaU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d7d630a-fe75-4b40-ae04-5cad5d53e4f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Title: ELI5-What is Deep learning?\n",
            "URL: https://www.reddit.com/r/MachineLearning/comments/1xjcxc/eli5what_is_deep_learning/\n",
            "Created UTC: 2014-02-10 18:26:46\n",
            "Upvotes: 45\n",
            "Number of comments: 27\n",
            "Author: chchan\n",
            "-----------\n",
            "Title: [R] Video of experiments from DeepMind's recent “Learning Agile Soccer Skills for a Bipedal Robot with Deep Reinforcement Learning” (OP3 Soccer) project\n",
            "URL: https://v.redd.it/jks9k9eo6uwa1\n",
            "Created UTC: 2023-04-29 14:50:41\n",
            "Upvotes: 2420\n",
            "Number of comments: 141\n",
            "Author: hardmaru\n",
            "-----------\n",
            "Title: [Discussion] Machine Learning is not just about Deep Learning\n",
            "URL: https://www.reddit.com/r/MachineLearning/comments/gonna8/discussion_machine_learning_is_not_just_about/\n",
            "Created UTC: 2020-05-22 17:26:34\n",
            "Upvotes: 669\n",
            "Number of comments: 191\n",
            "Author: ghost_agni\n",
            "-----------\n",
            "Title: [D] The $900,000 deep learning salary\n",
            "URL: https://www.reddit.com/r/MachineLearning/comments/16ce68p/d_the_900000_deep_learning_salary/\n",
            "Created UTC: 2023-09-07 12:08:20\n",
            "Upvotes: 143\n",
            "Number of comments: 122\n",
            "Author: blabboy\n",
            "-----------\n",
            "Title: [R] The Modern Mathematics of Deep Learning\n",
            "URL: https://www.reddit.com/r/MachineLearning/comments/najnjg/r_the_modern_mathematics_of_deep_learning/\n",
            "Created UTC: 2021-05-12 08:18:46\n",
            "Upvotes: 695\n",
            "Number of comments: 142\n",
            "Author: julbern\n",
            "-----------\n",
            "Title: [P] New textbook: Understanding Deep Learning\n",
            "URL: https://www.reddit.com/r/MachineLearning/comments/10jlq1q/p_new_textbook_understanding_deep_learning/\n",
            "Created UTC: 2023-01-23 19:54:38\n",
            "Upvotes: 330\n",
            "Number of comments: 65\n",
            "Author: SimonJDPrince\n",
            "-----------\n",
            "Title: M3 pro for Machine Learning/ Deep learning? [D]\n",
            "URL: https://www.reddit.com/r/MachineLearning/comments/17lw3gu/m3_pro_for_machine_learning_deep_learning_d/\n",
            "Created UTC: 2023-11-02 05:01:56\n",
            "Upvotes: 0\n",
            "Number of comments: 46\n",
            "Author: deadengineerssociety\n",
            "-----------\n",
            "Title: [R] Statistical vs Deep Learning forecasting methods\n",
            "URL: https://www.reddit.com/r/MachineLearning/comments/z9vbw7/r_statistical_vs_deep_learning_forecasting_methods/\n",
            "Created UTC: 2022-12-01 18:25:01\n",
            "Upvotes: 310\n",
            "Number of comments: 75\n",
            "Author: fedegarzar\n",
            "-----------\n",
            "Title: [N] Andrew Ng announces new Deep Learning specialization on Coursera\n",
            "URL: https://medium.com/@andrewng/deeplearning-ai-announcing-new-deep-learning-courses-on-coursera-43af0a368116\n",
            "Created UTC: 2017-08-08 15:20:42\n",
            "Upvotes: 1047\n",
            "Number of comments: 186\n",
            "Author: a19n\n",
            "-----------\n",
            "Title: [P] [R] Deep Learning Classifier for Sex Positions\n",
            "URL: https://www.reddit.com/r/MachineLearning/comments/va0p9u/p_r_deep_learning_classifier_for_sex_positions/\n",
            "Created UTC: 2022-06-11 16:08:11\n",
            "Upvotes: 414\n",
            "Number of comments: 94\n",
            "Author: rlesii\n",
            "-----------\n",
            "Title: [D] best advanced books of deep learning?\n",
            "URL: https://www.reddit.com/r/MachineLearning/comments/197r6o0/d_best_advanced_books_of_deep_learning/\n",
            "Created UTC: 2024-01-16 02:12:50\n",
            "Upvotes: 64\n",
            "Number of comments: 35\n",
            "Author: toxfu\n",
            "-----------\n",
            "Title: [R] Millions of new materials discovered with deep learning\n",
            "URL: https://www.reddit.com/r/MachineLearning/comments/186w67m/r_millions_of_new_materials_discovered_with_deep/\n",
            "Created UTC: 2023-11-29 18:19:08\n",
            "Upvotes: 209\n",
            "Number of comments: 35\n",
            "Author: RobbinDeBank\n",
            "-----------\n",
            "Title: [R] Is Deep Learning Suitable for Time Series Forecasting?\n",
            "URL: https://www.reddit.com/r/MachineLearning/comments/12hjh4m/r_is_deep_learning_suitable_for_time_series/\n",
            "Created UTC: 2023-04-10 13:53:09\n",
            "Upvotes: 78\n",
            "Number of comments: 88\n",
            "Author: nkafr\n",
            "-----------\n",
            "Title: [N] New book by Bishop: Deep Learning Foundations and Concepts\n",
            "URL: https://www.reddit.com/r/MachineLearning/comments/18pp37w/n_new_book_by_bishop_deep_learning_foundations/\n",
            "Created UTC: 2023-12-24 06:09:16\n",
            "Upvotes: 117\n",
            "Number of comments: 31\n",
            "Author: total-expectation\n",
            "-----------\n",
            "Title: [D] Is there a theory of Deep Learning?\n",
            "URL: https://www.reddit.com/r/MachineLearning/comments/13lwjcw/d_is_there_a_theory_of_deep_learning/\n",
            "Created UTC: 2023-05-19 14:10:13\n",
            "Upvotes: 131\n",
            "Number of comments: 51\n",
            "Author: finitearth\n",
            "-----------\n"
          ]
        }
      ],
      "source": [
        "import praw\n",
        "import datetime\n",
        "\n",
        "# Initialize Reddit instance with your credentials\n",
        "reddit = praw.Reddit(client_id='Rm51RgkkORs6zhyg23rNcg', client_secret='G4yNrSb24cbkLUXm5frLVOxjji18Bg', user_agent='test')\n",
        "\n",
        "def fetch_posts_from_subreddit(subreddit_name, search_keyword, limit=1000):\n",
        "    fetched_posts = []\n",
        "    subreddit = reddit.subreddit(subreddit_name)\n",
        "\n",
        "    # Iterate through submissions in the subreddit\n",
        "    for submission in subreddit.search(search_keyword, limit=limit):\n",
        "        post_info = {\n",
        "            'title': submission.title,\n",
        "            'url': submission.url,\n",
        "            'created_utc': datetime.datetime.utcfromtimestamp(submission.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n",
        "            'upvotes': submission.score,\n",
        "            'num_comments': submission.num_comments,\n",
        "            'author': str(submission.author)\n",
        "        }\n",
        "        fetched_posts.append(post_info)\n",
        "\n",
        "    return fetched_posts\n",
        "\n",
        "\n",
        "target_subreddit = 'machinelearning'\n",
        "search_query = 'deep learning'\n",
        "retrieved_posts = fetch_posts_from_subreddit(target_subreddit, search_query, limit=1000)\n",
        "\n",
        "for post in retrieved_posts:\n",
        "    print(\"Title:\", post['title'])\n",
        "    print(\"URL:\", post['url'])\n",
        "    print(\"Created UTC:\", post['created_utc'])\n",
        "    print(\"Upvotes:\", post['upvotes'])\n",
        "    print(\"Number of comments:\", post['num_comments'])\n",
        "    print(\"Author:\", post['author'])\n",
        "    print(\"-----------\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55W9AMdXCSpV"
      },
      "source": [
        "## Question 4B (10 Points)\n",
        "If you encounter challenges with Question-4 web scraping using Python, employ any online tools such as ParseHub or Octoparse for data extraction. Introduce the selected tool, outline the steps for web scraping, and showcase the final output in formats like CSV or Excel.\n",
        "\n",
        "\n",
        "\n",
        "Upload a document (Word or PDF File) in any shared storage (preferably UNT OneDrive) and add the publicly accessible link in the below code cell.\n",
        "\n",
        "Please only choose one option for question 4. If you do both options, we will grade only the first one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I57NXsauCec2"
      },
      "outputs": [],
      "source": [
        "# write your answer here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "sZOhks1dXWEe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on Web Scraping and Data Collection**\n",
        "\n",
        "\n",
        "\n",
        "Please share your thoughts and feedback on the web scraping and data collection exercises you have completed in this assignment. Consider the following points in your response:\n",
        "\n",
        "\n",
        "\n",
        "Learning Experience: Describe your overall learning experience in working on web scraping tasks. What were the key concepts or techniques you found most beneficial in understanding the process of extracting data from various online sources?\n",
        "\n",
        "\n",
        "\n",
        "Challenges Encountered: Were there specific difficulties in collecting data from certain websites, and how did you overcome them? If you opted for the non-coding option, share your experience with the chosen tool.\n",
        "\n",
        "\n",
        "\n",
        "Relevance to Your Field of Study: How might the ability to gather and analyze data from online sources enhance your work or research?\n",
        "\n",
        "**(no grading of your submission if this question is left unanswered)**"
      ],
      "metadata": {
        "id": "eqmHVEwaWhbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Write your response here.\n",
        "Learning Experience:\n",
        "Web scraping taught valuable skills like understanding HTML/CSS, using libraries like BeautifulSoup, and handling HTTP requests effectively.\n",
        "\n",
        "Challenges Encountered:\n",
        "Dynamic content and legal concerns were challenges.\n",
        "\n",
        "Relevance to Your Field of Study:\n",
        "Web scraping enhances response quality for AI models, provides large datasets for research in various fields, and supplies training data for machine learning tasks.\n",
        "'''"
      ],
      "metadata": {
        "id": "akAVJn9YBTQT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "55W9AMdXCSpV"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}